{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16613f46",
   "metadata": {},
   "source": [
    "# FAISS 비동기(Asynchronous)\n",
    "\n",
    "본 튜토리얼은 FAISS DB 를 비동기로 활용하는 방법을 다룹니다.\n",
    "\n",
    "더욱 자세한 사용법은 [Faiss 공식 문서](https://faiss.ai/)를 참조하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bc0ae",
   "metadata": {},
   "source": [
    "faiss-gpu 또는 faiss-cpu 라이브러리를 설치합니다.\n",
    "\n",
    "- CUDA 7.5 이상을 지원하는 GPU가 있는 경우 `faiss-gpu`를 설치합니다.\n",
    "- GPU가 없거나 CPU에서 실행하려는 경우 `faiss-cpu`를 설치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e90789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  faiss-gpu # CUDA 7.5 이상을 지원하는 GPU용 설치\n",
    "# 또는\n",
    "# %pip install --upgrade --quiet  faiss-cpu # CPU용 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548bd35",
   "metadata": {},
   "source": [
    "혹은 `dot_env` 라이브러리를 사용하여 `.env` 파일을 만들어 환경 변수로 설정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c0a46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455af5a1",
   "metadata": {},
   "source": [
    "추적을 위한 LangSmith 설정(필수는 아닙니다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGCHAIN_TRACING_V2 환경 변수를 \"true\"로 설정합니다.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# LANGCHAIN_API_KEY 환경 변수를 getpass.getpass() 함수를 통해 입력받은 값으로 설정합니다.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN API KEY 입력\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dae6df",
   "metadata": {},
   "source": [
    "- `TextLoader`를 사용하여 텍스트 데이터를 로드합니다.\n",
    "- `CharacterTextSplitter`를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.\n",
    "- `OpenAIEmbeddings`를 사용하여 문서 임베딩을 생성합니다.\n",
    "- `FAISS` 벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- `AVX2`: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우 `AVX2` 를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.\n",
    "- 필요한 경우 `os.environ['FAISS_NO_AVX2'] = '1'` 코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7b919",
   "metadata": {},
   "source": [
    "샘플 데이터(`appendix-keywords.txt`) 일부 파일의 내용을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb983126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# data/appendix-keywords.txt 파일 내용을 읽어서 file 변수에 저장합니다.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# file 변수에 저장된 내용을 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461b4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.\n",
    "# import os\n",
    "#\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = await FAISS.afrom_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b122c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n"
     ]
    }
   ],
   "source": [
    "# 검색할 쿼리 설정\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# 쿼리와 유사한 문서 검색\n",
    "docs = await db.asimilarity_search(query)\n",
    "\n",
    "# 검색된 문서의 내용 출력\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d921a98",
   "metadata": {},
   "source": [
    "## 점수에 기반한 유사도 검색\n",
    "\n",
    "FAISS에는 몇 가지 특정 메서드가 있습니다.\n",
    "\n",
    "그 중 하나는 `similarity_search_with_score`로, 문서뿐만 아니라 쿼리와 문서 간의 거리 점수도 반환할 수 있습니다.\n",
    "\n",
    "반환되는 거리 점수는 L2 거리입니다.\n",
    "\n",
    "따라서 점수가 낮을수록 더 좋은 결과입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fcd9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken', metadata={'source': './data/appendix-keywords.txt'}),\n",
       " 0.29650298)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.\n",
    "docs_and_scores = await db.asimilarity_search_with_score(query)\n",
    "\n",
    "# 검색 결과 중 가장 유사도가 높은 문서와 점수를 가져옵니다.\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26382d",
   "metadata": {},
   "source": [
    "`similarity_search_by_vector` 함수를 사용하면 주어진 임베딩 벡터와 유사한 문서를 검색할 수 있습니다.\n",
    "\n",
    "이 함수는 문자열 대신 **임베딩 벡터** 를 매개변수로 받아들입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0980a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색할 쿼리 설정\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "# 쿼리를 임베딩 벡터로 변환합니다.\n",
    "embedding_vector = await embeddings.aembed_query(query)\n",
    "# 임베딩 벡터를 사용하여 유사도 검색을 수행하고 문서와 점수를 반환합니다.\n",
    "docs_and_scores = await db.asimilarity_search_by_vector(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2288d3",
   "metadata": {},
   "source": [
    "## 저장 및 로드\n",
    "\n",
    "FAISS 인덱스를 저장하고 불러올 수도 있습니다.\n",
    "\n",
    "이는 인덱스를 사용할 때마다 매번 다시 생성할 필요가 없어 유용합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917c1f",
   "metadata": {},
   "source": [
    "- `db.save_local(\"저장할 인덱스 이름\")`를 사용하여 FAISS 인덱스를 로컬 디렉토리에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c69f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에 \"MY_ASYNC_DB_INDEX\"라는 이름으로 데이터베이스를 저장합니다.\n",
    "DB_INDEX = \"MY_ASYNC_DB_INDEX\"\n",
    "db.save_local(DB_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bd181",
   "metadata": {},
   "source": [
    "- `FAISS.load_local(\"불러올 인덱스 이름\", embeddings)`를 사용하여 저장된 FAISS 인덱스를 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28d8991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken', metadata={'source': './data/appendix-keywords.txt'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.\n",
    "new_db = FAISS.load_local(DB_INDEX, embeddings,\n",
    "                          allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 비동기적으로 검색하여 docs 변수에 할당합니다.\n",
    "docs = await new_db.asimilarity_search(query)\n",
    "\n",
    "docs[0]  # 검색 결과 중 가장 유사한 문서를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a3c32",
   "metadata": {},
   "source": [
    "# 바이트 형태로 직렬화/역직렬화\n",
    "\n",
    "FAISS 인덱스를 직렬화(Serializing)하고 역직렬화(De-Serializing)하는 데에는 `pickle` 함수를 사용하여 저장할 수 있습니다. 하지만, 만약 90MB 크기의 임베딩 모델(예: `sentence-transformers/all-MiniLM-L6-v2` 또는 다른 모델)을 사용한다면, 결과로 생성되는 pickle 파일의 크기는 90MB 이상이 될 것입니다. 이는 모델의 크기 또한 전체 크기에 포함되기 때문입니다.\n",
    "\n",
    "따라서, 아래의 함수들을 사용하는 것이 좋습니다. 이 함수들은 **FAISS 인덱스만 직렬화** 하므로 크기가 훨씬 작아집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a679a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teddy/miniconda3/envs/py-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# HuggingFaceEmbeddings 사용하여 임베딩을 생성합니다.\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(docs, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "870c7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 직렬화합니다.\n",
    "serialized_db_index = db.serialize_to_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ee24f",
   "metadata": {},
   "source": [
    "직렬화된 `serialized_db_index` 의 사이즈를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c5f7ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60.81 KB'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 사이즈 측정을 위한 함수를 정의합니다.\n",
    "\n",
    "\n",
    "def get_size(path):\n",
    "    size = sys.getsizeof(path)\n",
    "    if size < 1024:\n",
    "        return f\"{size} bytes\"\n",
    "    elif size < pow(1024, 2):\n",
    "        return f\"{round(size/1024, 2)} KB\"\n",
    "    elif size < pow(1024, 3):\n",
    "        return f\"{round(size/(pow(1024,2)), 2)} MB\"\n",
    "    elif size < pow(1024, 4):\n",
    "        return f\"{round(size/(pow(1024,3)), 2)} GB\"\n",
    "\n",
    "\n",
    "# 직렬화된 FAISS 인덱스의 사이즈를 출력합니다.\n",
    "get_size(serialized_db_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac798b",
   "metadata": {},
   "source": [
    "이번에는 `deserialize_from_bytes` 함수로 역직렬화를 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5c1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화된 인덱스를 로드합니다.\n",
    "deserialized_db = FAISS.deserialize_from_bytes(\n",
    "    embeddings=hf_embeddings,  # 직렬화 할 때의 임베딩과 동일하게 지정\n",
    "    serialized=serialized_db_index,  # 직렬화된 인덱스\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b81226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer', metadata={'source': './data/appendix-keywords.txt'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.\n",
    "docs = deserialized_db.similarity_search(query)\n",
    "\n",
    "# 문서 리스트의 첫 번째 문서를 가져옵니다.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748ffbe",
   "metadata": {},
   "source": [
    "## 병합(merge)\n",
    "\n",
    "FAISS 벡터 저장소를 병합하는 것도 가능합니다.\n",
    "\n",
    "두 개의 FAISS 벡터 저장소를 하나로 합칠 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451ca25",
   "metadata": {},
   "source": [
    "- `FAISS.afrom_texts` 메서드를 사용하여 두 개의 FAISS 데이터베이스 `db1`과 `db2`를 비동기적으로 생성합니다.\n",
    "- `db1`은 `[\"foo\"]` 텍스트 리스트로부터 생성되며, `embeddings`를 사용하여 벡터화됩니다.\n",
    "- `db2`는 `[\"bar\"]` 텍스트 리스트로부터 생성되며, 동일한 `embeddings`를 사용하여 벡터화됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73053ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# db1 생성\n",
    "db1 = FAISS.from_texts([\"LangChain DB 1의 내용\"], embeddings)\n",
    "# db2 생성\n",
    "db2 = FAISS.from_texts([\"DB 2 의 내용\"], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1597e8e",
   "metadata": {},
   "source": [
    "`db1` 의 내용 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ac5e999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5ad3475d-3a95-469c-b868-8ab93b644708': Document(page_content='LangChain DB 1의 내용')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 벡터 데이터베이스의 내부 문서 저장소 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbfbbc",
   "metadata": {},
   "source": [
    "`db2` 의 내용 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "003fbadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3439056f-c28f-4366-baa0-c79a949378c6': Document(page_content='DB 2 의 내용')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db2 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db2.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2ffdb",
   "metadata": {},
   "source": [
    "`db1` 객체의 `merge_from` 메서드를 사용하여 `db2` 객체의 내용을 `db1`에 병합합니다.\n",
    "\n",
    "- `merge_from` 메서드는 `db2`의 데이터를 `db1`에 병합하는 기능을 수행합니다.\n",
    "- 병합 작업 후에는 `db1`에 `db2`의 데이터가 포함됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f617f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.merge_from(db2)  # db1에 db2를 병합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3fb7a2",
   "metadata": {},
   "source": [
    "병합된 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65077a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5ad3475d-3a95-469c-b868-8ab93b644708': Document(page_content='LangChain DB 1의 내용'),\n",
       " '3439056f-c28f-4366-baa0-c79a949378c6': Document(page_content='DB 2 의 내용')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdf5f3",
   "metadata": {},
   "source": [
    "## 필터링\n",
    "\n",
    "FAISS vectorstore는 필터링 기능도 지원할 수 있습니다.\n",
    "\n",
    "FAISS는 기본적으로 필터링을 지원하지 않기 때문에, 이를 수동으로 처리해야 합니다.\n",
    "\n",
    "**방식**\n",
    "\n",
    "1. 이는 먼저 `k`보다 더 많은 결과를 가져온 다음 필터링하는 방식으로 이루어집니다.\n",
    "\n",
    "2. 이 필터는 메타데이터 dict를 입력으로 받아 bool을 반환하는 호출 가능한 함수이거나, 누락된 각 키는 무시되고 존재하는 각 키는 값 목록에 있어야 하는 메타데이터 dict입니다.\n",
    "\n",
    "또한 검색 메서드를 호출할 때 `fetch_k` 매개변수를 설정하여 필터링 전에 가져올 문서 수를 지정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "list_of_documents = [\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"bar\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"barbar\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"bar burr\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    # 페이지 내용이 \"bar bruh\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]\n",
    "# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "\n",
    "# \"foo\"와 유사한 문서를 검색하고 점수와 함께 결과 반환\n",
    "results_with_scores = db.similarity_search_with_score(\"foo\")\n",
    "\n",
    "for doc, score in results_with_scores:  # 검색 결과를 반복하면서\n",
    "    # 각 문서의 내용, 메타데이터, 점수를 출력\n",
    "    print(\n",
    "        f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdd954d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "list_of_documents = [\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"bar\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"barbar\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"bar burr\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    # 페이지 내용이 \"bar bruh\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]\n",
    "# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "results_with_scores = db.asimilarity_search_with_score(\n",
    "    \"foo\"\n",
    ")  # \"foo\"와 유사한 문서를 검색하고 점수와 함께 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac2191",
   "metadata": {},
   "source": [
    "이제 동일한 쿼리 호출을 수행하지만, `page = 1`인 경우만 필터링합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 검색을 수행하고 결과를 점수와 함께 가져옵니다. 필터로 page가 1인 문서만 검색합니다.\n",
    "results_with_scores = await db.asimilarity_search_with_score(\"foo\", filter=dict(page=1))\n",
    "for (\n",
    "    doc,\n",
    "    score,\n",
    ") in results_with_scores:  # 검색 결과를 반복하면서 각 문서와 점수를 가져옵니다.\n",
    "    # 문서의 내용, 메타데이터, 점수를 출력합니다.\n",
    "    print(\n",
    "        f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ff90d",
   "metadata": {},
   "source": [
    "## Max Marginal Relevance (MMR)\n",
    "\n",
    "`MMR(Maximal Marginal Relevance)` 방식은 쿼리에 대한 관련 항목을 검색할 때 중복을 피하는 방법 중 하나입니다. 단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 검색된 항목들 사이에 **관련성과 다양성 사이의 균형을 보장**합니다. 이는 **자주 발생할 수 있는, 매우 유사한 항목들만이 검색되는 상황을 방지** 하는 데에 유용합니다.\n",
    "\n",
    "예를 들어, 특정 주제에 대해 정보를 찾고 있다고 가정해봅시다. 가장 관련성 높은 문서만을 반환하는 시스템은 비슷비슷한 정보를 담은 문서들을 제공할 수 있습니다. 하지만, MMR 방식을 사용하면, 검색된 문서들이 해당 주제에 대해 서로 다른 관점이나 새로운 정보를 제공하도록 합니다. 이로써 사용자는 주제에 대해 보다 폭넓은 이해를 할 수 있게 됩니다.\n",
    "\n",
    "MMR은 두 가지 주요 요소, 즉 쿼리에 대한 문서의 관련성과 **이미 선택된 문서들과의 차별성을 동시에 고려** 합니다.\n",
    "\n",
    "1. 첫째로, 쿼리와의 관련성이 높은 문서를 찾는 것이 중요합니다.\n",
    "2. 둘째로, 이미 선택된 문서들과는 다른 새로운 정보나 관점을 제공하는 문서를 찾는 것입니다.\n",
    "\n",
    "이 두 가지 요소의 균형을 맞추는 것이 MMR의 핵심입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e13ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n",
      "Content: bar, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"를 검색어로 사용하여 최대 한계 관련성 검색을 수행하고, 필터로 page가 1인 문서만 검색합니다.\n",
    "results = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\n",
    "for doc in results:  # 검색 결과를 반복합니다.\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52c0bf",
   "metadata": {},
   "source": [
    "`similarity_search` 함수를 호출할 때 `fetch_k` 매개변수를 설정하는 방법에 대한 예시입니다.\n",
    "\n",
    "일반적으로 `fetch_k` 매개변수는 `k` 매개변수보다 훨씬 큰 값으로 설정하는 것이 좋습니다. 그 이유는 `fetch_k` 매개변수가 필터링 전에 가져올 문서의 수를 나타내기 때문입니다.\n",
    "\n",
    "만약 `fetch_k`를 낮은 값으로 설정하면, 필터링할 문서가 충분하지 않을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47acaee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"와 유사한 문서를 검색하고, 'page' 메타데이터가 1인 문서만 필터링하여 가장 유사한 1개의 문서를 반환하되, 4개의 문서를 가져옵니다.\n",
    "results = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\n",
    "for doc in results:  # 검색 결과를 반복하면서 각 문서에 대해 다음을 수행합니다.\n",
    "    # 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2131ee4",
   "metadata": {},
   "source": [
    "## 문서 삭제\n",
    "\n",
    "벡터 저장소에 저장된 문서를 삭제할 수도 있습니다. 이때 삭제할 문서의 ID는 DB 에 저장된 ID와 일치해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded07e0",
   "metadata": {},
   "source": [
    "`db.delete()` 메서드를 사용하여 문서를 삭제합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01757d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'660aafd1-5952-4caf-b55d-e7cc3db10712'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d96635ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 0에 해당하는 문서 저장소 ID를 사용하여 데이터베이스에서 문서를 삭제합니다.\n",
    "db.delete([db.index_to_docstore_id[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
