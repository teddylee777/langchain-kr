{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27328084",
   "metadata": {},
   "source": [
    "### ê²€ìƒ‰ë„êµ¬: Tavily Search\n",
    "\n",
    "LangChainì—ëŠ” Tavily ê²€ìƒ‰ ì—”ì§„ì„ ë„êµ¬ë¡œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‚´ì¥ ë„êµ¬ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Tavily Search ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” API KEYë¥¼ ë°œê¸‰ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "- [Tavily Search API ë°œê¸‰ë°›ê¸°](https://app.tavily.com/sign-in)\n",
    "\n",
    "ë°œê¸‰ ë°›ì€ API KEY ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œì˜ ì£¼ì„ì„ í’€ê³  ë°œê¸‰ë°›ì€ API KEY ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d1b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TAVILY API KEYë¥¼ ê¸°ì…í•©ë‹ˆë‹¤.\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"TAVILY API KEY ì…ë ¥\"\n",
    "\n",
    "# ë””ë²„ê¹…ì„ ìœ„í•œ í”„ë¡œì íŠ¸ëª…ì„ ê¸°ì…í•©ë‹ˆë‹¤.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AGENT TUTORIAL\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e82eaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY ì •ë³´ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecdb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TavilySearchResults í´ë˜ìŠ¤ë¥¼ langchain_community.tools.tavily_search ëª¨ë“ˆì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# TavilySearchResults í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n",
    "# k=5ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ 5ê°œê¹Œì§€ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤\n",
    "search = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24519b31",
   "metadata": {},
   "source": [
    "### Agent ê°€ ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡ ì •ì˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1984dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools ë¦¬ìŠ¤íŠ¸ì— search ë„êµ¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d460016",
   "metadata": {},
   "source": [
    "## Agent with smaller model\n",
    "\n",
    "- reference: https://github.com/langchain-ai/langchain/blob/master/templates/neo4j-semantic-ollama/neo4j_semantic_ollama/agent.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd2c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_log_to_messages\n",
    "from langchain.agents.output_parsers import (\n",
    "    ReActJsonSingleInputOutputParser,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "local_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"cognitivecomputations/dolphin-2.9-llama3-8b-gguf\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "tools = [search]\n",
    "\n",
    "llm_with_tools = local_llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123e345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "chat_model_with_stop = local_llm.bind(stop=[\"Observation\", \"\\nObservation\", \"\\nê´€ì¸¡\"])\n",
    "\n",
    "# Inspiration taken from hub.pull(\"hwchase17/react-json\")\n",
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherise, you have access to the following tools:\n",
    "\n",
    "{render_text_description_and_args(tools).replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
    "and a `action_input` key (with the input to the tool going here).\n",
    "The only values that should be in the \"action\" field are: {[t.name for t in tools]}\n",
    "The $JSON_BLOB should only contain a SINGLE action, \n",
    "do NOT return a list of multiple actions.\n",
    "Here is an example of a valid $JSON_BLOB:\n",
    "```\n",
    "{{{{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}}}}\n",
    "```\n",
    "The $JSON_BLOB must always be enclosed with triple backticks!\n",
    "\n",
    "ALWAYS use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action:```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: the result of the action... \n",
    "(this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            system_message,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]):\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d418fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_messages(x[\"intermediate_steps\"]),\n",
    "        \"chat_history\": lambda x: (\n",
    "            _format_chat_history(x[\"chat_history\"]) if x.get(\n",
    "                \"chat_history\") else []\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | ReActJsonSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# Add typing for input\n",
    "class AgentInput(BaseModel):\n",
    "    input: str\n",
    "    chat_history: List[Tuple[str, str]] = Field(\n",
    "        ..., extra={\"widget\": {\"type\": \"chat\", \"input\": \"input\", \"output\": \"output\"}}\n",
    "    )\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ").with_types(input_type=AgentInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ecb5efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<|im_start|>system\n",
      "Perform the task to the best of your ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherise, you have access to the following tools:\n",
      "\n",
      "tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query., args: {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
      "and a `action_input` key (with the input to the tool going here).\n",
      "The only values that should be in the \"action\" field are: ['tavily_search_results_json']\n",
      "The $JSON_BLOB should only contain a SINGLE action, \n",
      "do NOT return a list of multiple actions.\n",
      "Here is an example of a valid $JSON_BLOB:\n",
      "```\n",
      "{\n",
      "    \"action\": $TOOL_NAME,\n",
      "    \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "The $JSON_BLOB must always be enclosed with triple backticks!\n",
      "\n",
      "ALWAYS use the following format:\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action:```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: the result of the action... \n",
      "(this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ë¥¼ ì›¹ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Question: íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ë¥¼ ì›¹ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "Thought: I need to search for the phone number of Pan-gyo Mong-chun-hen using a web search engine. This will provide accurate and trusted results.\n",
      "Action:```{\n",
      "     \"action\": \"tavily_search_results_json\",\n",
      "     \"action_input\": {\n",
      "         \"query\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸\"\n",
      "     }\n",
      "}```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ë‹µë³€: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ë¥¼ ì›¹ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Question: íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ë¥¼ ì›¹ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "Thought: I need to search for the phone number of Pan-gyo Mong-chun-hen using a web search engine. This will provide accurate and trusted results.\n",
      "Action:```{\n",
      "     \"action\": \"tavily_search_results_json\",\n",
      "     \"action_input\": {\n",
      "         \"query\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸\"\n",
      "     }\n",
      "}```\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì²­ í›„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ë¥¼ ì›¹ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")\n",
    "print(f'ë‹µë³€: {response[\"output\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69fb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "def format_search_result(search_result):\n",
    "    return \" \".join([r[\"content\"] for r in search_result])\n",
    "\n",
    "\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95471cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = (\n",
    "    agent_executor\n",
    "    | itemgetter(\"output\")\n",
    "    | ReActJsonSingleInputOutputParser()\n",
    "    | tool_executor\n",
    "    | format_search_result\n",
    ")\n",
    "\n",
    "search_agent_parallel = RunnableParallel(\n",
    "    context=search_agent, question=itemgetter(\"input\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d003997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<|im_start|>system\n",
      "Perform the task to the best of your ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherise, you have access to the following tools:\n",
      "\n",
      "tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query., args: {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
      "and a `action_input` key (with the input to the tool going here).\n",
      "The only values that should be in the \"action\" field are: ['tavily_search_results_json']\n",
      "The $JSON_BLOB should only contain a SINGLE action, \n",
      "do NOT return a list of multiple actions.\n",
      "Here is an example of a valid $JSON_BLOB:\n",
      "```\n",
      "{\n",
      "    \"action\": $TOOL_NAME,\n",
      "    \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "The $JSON_BLOB must always be enclosed with triple backticks!\n",
      "\n",
      "ALWAYS use the following format:\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action:```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: the result of the action... \n",
      "(this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ì›¹ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì¤˜.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Question: íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ì›¹ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì¤˜.\n",
      "Thought: The user wants me to search for the phone number of Pan-gyo Mong-chun-hon on the web. I will use the tavily_search_results_json tool to perform a search and find the relevant information.\n",
      "Action:```{\n",
      "     \"action\": \"tavily_search_results_json\",\n",
      "     \"action_input\": {\n",
      "         \"query\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸\"\n",
      "     }\n",
      "}```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': \"ì¤‘ì‹ | íŒêµ ëª½ì¤‘í—Œ íŒêµì  ... - íŒêµ í…Œí¬ì› íƒ€ì›Œ ê±´ë¬¼ ë‚´ ì§€í•˜ ì£¼ì°¨ì¥ì„ ì´ìš© ë¶€íƒë“œë¦¬ë©° , 2ì‹œê°„ ì´ìš©ê°€ëŠ¥ í•©ë‹ˆë‹¤. ... ì „í™”ë²ˆí˜¸ 031-601-7574. ì˜ëª»ëœ ì •ë³´ê°€ ìˆë‚˜ìš”? ìˆ˜ì •ì´ í•„ìš”í•˜ê±°ë‚˜ íì í•œ ë§¤ì¥ì´ë¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”! ì €ì¥ 1,225. ì˜ˆì•½í•˜ê¸° ... *2ì¸ ì´ìƒ ì£¼ë¬¸ ì‹œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\nëƒ‰ì±„ë¥˜\\níƒ•ë¥˜\\nì¡í’ˆë¥˜\\në°”ë‹·ê°€ì¬Â·ìƒˆìš°ë¥˜\\nì‡ ê³ ê¸°ë¥˜\\në¼ì§€ê³ ê¸°ë¥˜\\në‹­ê³ ê¸°ë¥˜\\nêµ¬ì´ë¥˜\\nì±„ì†ŒÂ·ë‘ë¶€ë¥˜\\në©´ë¥˜\\në°¥ë¥˜\\nSteamed Dim Sum\\nì¡°ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ë‹­ìœ¡ìˆ˜ìš© ë‹­ì€ êµ­ë‚´ì‚°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n Chef-Recommended\\nSpicy Dish\\nDim Sum Special Course\\nLUNCH - 75,000 (1äºº)\\nDINNER - 95,000 (1äºº)\\nì¡°ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ë‹­ìœ¡ìˆ˜ìš© ë‹­ì€ êµ­ë‚´ì‚°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n í• ì¸ ì œê³µ ì¢…ë£Œ\\nê¸°í”„íŠ¸ ì¹´ë“œ\\nëª½ì¤‘í—Œ ë¸Œëœë“œ ì „ìš© ê¸°í”„íŠ¸ ì¹´ë“œ\\nChef â€™s Special Course / All Day\\n180,000 (1äºº)\\nì¡°ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ë‹­ìœ¡ìˆ˜ìš© ë‹­ì€ êµ­ë‚´ì‚°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n ë§¤ì¥ì •ë³´\\nëŒ€ì¤‘êµí†µ\\nì œíœ´ ìƒí’ˆê¶Œ\\nCJìƒí’ˆê¶Œ, CJì™¸ì‹ì „ìš©ìƒí’ˆê¶Œ *ì§€ë¥˜ ìƒí’ˆê¶Œì— í•œí•´ ì ìš© ê°€ëŠ¥\\nì œíœ´ ì¹´ë“œ\\ní˜„ëŒ€ì¹´ë“œ ë¸”ë™ / í¼í”Œ / ë ˆë“œ / ê·¸ë¦° / í•‘í¬ / ëŒ€í•œí•­ê³µ ë”í¼ìŠ¤íŠ¸ ì¹´ë“œì˜ â€˜í´ëŸ½ ê³ ë©” ì„œë¹„ìŠ¤â€™ ìš´ì˜ ì¢…ë£Œë¡œ, 2024ë…„ 1ì›” 1ì¼ë¶€í„° ëª½ì¤‘í—Œ ì „ ë§¤ì¥ 10% í˜„ì¥ Chef-Recommended\\nSpicy Dish\\nFried Dim Sum\\nì¡°ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ë‹­ìœ¡ìˆ˜ìš© ë‹­ì€ êµ­ë‚´ì‚°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n ğŸ“ ì „í™”ë²ˆí˜¸: 031-601-7574. ğŸŒ í™ˆí˜ì´ì§€: ëª½ì¤‘í—Œ ê³µì‹ í™ˆí˜ì´ì§€ ì˜¤ëŠ˜ì€ ê°€ì¡±ëª¨ì„ ì¥ì†Œë¡œ ì¶”ì²œí•˜ëŠ” ë¶„ë‹¹ ì¤‘ì‹ë‹¹ ëª½ì¤‘í—Œì„ ì†Œê°œí•´ë´…ë‹ˆë‹¤. ì €ëŠ” ê°€ì¡±ëª¨ì„ì´ ìˆì–´ì„œ ë£¸ì´ìˆëŠ” ì‹ë‹¹ì„ ì°¾ë‹¤ê°€ íŒêµì— ëª½ì¤‘í—Œì´ ìˆëŠ”ê±¸ ì•Œê³  ì˜ˆì•½ì„ í•œë‹¬ì „ì— ì§„í–‰í–ˆì–´ìš” ã…‹ã…‹\\n\\u200b\\në©”ì¸ ë©”ë‰´íŒ ë³´ì‹¤ê²Œìš”\\n\\u200b\\n\\u200b\\nì‚¬ì§„ì„ ì°ë‹¤ê°€ ì§€ì³í•˜ë‹ˆê¹Œ\\nì•ì— ì¼í–‰ì´ ë©”ë‰´íŒì„ ë“¤ì–´ì¤¬ìŠµë‹ˆë‹¤\\nã…‹ã…‹ã…‹ã…‹ã…‹\\në¯¼ë§..\\n\\u200b\\nì €í¬ ì¼í–‰ì€\\në”¤ì„¬ 4ì¢…ë¥˜\\u200b\\ní•˜êµ(í†µìƒˆìš°) â‚©12,000\\nêµ¬ì±„êµ(ìƒˆìš°ì™€ ë¶€ì¶”) â‚©12,000\\nê·¼ì±„êµ(ìƒˆìš°, ë¼ì§€ê³ ê¸°, ìƒëŸ¬ë¦¬, ì¹ ë¦¬í˜í¼) â‚©12,000\\nìƒí•˜ì´ì†Œë¡±í¬(ìƒí•˜ì´ì‹ ë¼ì§€ê³ ê¸°) â‚©15,000\\nê·¸ë¦¬ê³ \\nì‚¼ì„ ì§¬ë½• â‚©18,000\\nì‚¬ì²œíƒ•ë©´ â‚©18,000\\nê²Œì‚´ë³¶ìŒë°¥ â‚©18,000\\nì„ ì£¼ë¬¸í–ˆìŠµë‹ˆë‹¤.\\n ì™œ ë”¤ì„¬ì „ë¬¸ì ì¸ì§€ ì•Œê² ë‹¤ëŠ” ã…ã…\\n\\u200b\\nì´ì–´ì„œ ë‚˜ì˜¨\\n\\u200b\\n\\u200b\\nì¼í–‰1ì˜ XOì†ŒìŠ¤ ê²Œì‚´ ë³¶ìŒë°¥\\n\\u200b\\n\\u200b\\nì¼í–‰2ì˜ ì‚¬ì²œíƒ•ë©´\\n\\u200b\\n\\u200b\\nì´ê±´ ì œê°€ ì£¼ë¬¸í•œ ì‚¼ì„ ì§¬ë½• ì…ë‹ˆë‹¤.\\n \\u200b\\nëª½ì¤‘í—Œ íŒêµì \\nê²½ê¸° ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬ ë¶„ë‹¹ë‚´ê³¡ë¡œ 131, íŒêµí…Œí¬ì›íƒ€ì›Œ 2ì¸µ\\në§¤ì¥ì „í™”ë²ˆí˜¸) 031-601-7574\\nì˜ì—…ì‹œê°„) 11:30 ~ 22:00\\n(ë¸Œë ˆì´í¬íƒ€ì„ 15:00 ~ 17:30)\\n\\u200b\\níŒêµí…Œí¬ì›íƒ€ì›ŒëŠ” íŒêµì—­ 1ë²ˆì¶œêµ¬ë¡œ ë‚˜ì˜¤ë©´\\në°”ë¡œ ë³´ì´ëŠ” ì´ê³³\\n\\u200b\\n\\u200b\\níˆ¬ì¸í”Œë ˆì´ìŠ¤ê°€ ìˆëŠ” ì´ ê±´ë¬¼ì´ ë˜ê² ìŠµë‹ˆë‹¤!!\\n \\u200b\\në”¤ì„¬ìœ¼ë¡œ ìœ ëª…í•œ 'ëª½ì¤‘í—Œ'ì€\\níŒêµì—­ ë°”ë¡œì•ì— ìˆëŠ”\\níŒêµí…Œí¬ì›íƒ€ì›Œ 2ì¸µì— ìœ„ì¹˜í•´ìˆêµ¬ìš”\\nëª‡ì£¼ì „ë¶€í„° ì˜¤í”ˆì˜ˆì • í˜„ìˆ˜ë§‰ì„ ë³´ê³ \\nê¸°ëŒ€ë¥¼ í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤ ã…‹ã…‹\\n\\u200b\\nì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ì€ ì‹ ê·œì˜¤í”ˆí•œì§€ 3ì¼ë°–ì— ì•ˆëœ ë”°ëˆë”°ëˆí•œ ì‹ ìƒë§›ì§‘ ì†Œê°œí•´ë“œë¦½ë‹ˆë‹¤ ã… ì´ë¯¸ ë°©ì†¡ìœ¼ë¡œ ìœ ëª…...\\nm.blog.naver.com\\níŒêµí…Œí¬ì›íƒ€ì›Œ ì •ë³´ëŠ” ìœ„ í¬ìŠ¤íŒ… ì°¸ê³ í•´ì£¼ì‹œê³ \\n\\u200b\\nê·¸ëŸ¼ ë§Œë‚˜ëŸ¬ ê°€ë³´ê² ìŠµë‹ˆë‹¤.\\n \\u200b\\nê¸°ì¡´ì— ì•Œê³ ìˆë˜ ê°€ê²©ì—ì„œ ë‹¤ ì¡°ê¸ˆì”© ì˜¬ëë”ë¼êµ¬ìš”\\n(2~3ì²œì›ì”©)\\n\\u200b\\n\\u200b\\nê¸°ë³¸ë°˜ì°¬ì´ì—ìš”\\n(ì§œì‚¬ì´ì™€ ë¬´, ë•…ì½©)\\n\\u200b\\n\\u200b\\në”¤ì„¬ 3ê°€ì§€(í•˜êµ, êµ¬ì±„êµ, ê·¼ì±„êµ)ëŠ”\\nê° 1ê°œì”© ë‚¨ê¸°ëŠ” ë°±ì£¼\\n500ml 75,000\\në†í–¥í˜• 34% ì—°íƒœì§€ë°©ì˜ ìˆ˜ìˆ˜ë¥¼ ì›ë£Œë¡œ ë§Œë“  í–¥ì´ ì¢‹ê³  ë¶€ë“œëŸ¬ìš´ ë°±ì£¼\\n500ml 65,000\\n250ml 38,000\\n125ml 28,000\\nCHAMPAGNE Â· SPARKLING WINE\\n350,000\\n250,000\\n160,000\\n90,000\\n70,000\\nWHITE WINE\\n170,000\\n110,000\\n95,000\\n90,000\\n85,000\\n75,000\\nRED WINE\\n380,000\\n180,000\\n150,000\\n130,000\\n120,000\\n100,000\\n95,000\\n90,000\\n80,000\\n75,000\\n70,000\\nGLASS WINE\\n15,000\\n15,000\\nDISTILLED LIQUOR\\n375ml 35,000\\n375ml 35,000\\nBEER\\n320ml 11,000\\n640ml 14,000\\n580ml 11,000\\n600ml 12,000\\n610ml 12,000\\n330ml 8,000\\n330ml 8,000\\nSOFT DRINK\\n7,000\\n7,000\\n5,000\\n5,000\\n4,500\\n8,000\\nCOFFEE\\nhot 5,000\\nIced 6,000\\nhot 6,000\\nIced 7,000\\nhot 5,000\\nDim Sum Special Course\\nLUNCH - 70,000 (1äºº)\\nDINNER - 90,000 (1äºº)\\n Chef-Recommended\\nSpicy Dish\\nëŒ€ë§Œ\\nê¸ˆë¬¸í–¥í˜• 58% ìµœìƒì˜ ìˆ˜ì§ˆê³¼ ê¸°í›„ë¥¼ ê°€ì§„ ê¸ˆë¬¸ë„ì—ì„œ ìë€ ìˆ˜ìˆ˜ë¥¼ ì—„ì„ í•˜ì—¬ ì œì¡°í•œ ìˆ ë¡œ, ë¶€ë“œëŸ½ê³  ê°ë¯¸ë¡œìš´ ë§›ì´ ìš°ìˆ˜í•œ ë°±ì£¼\\n600ml 180,000\\n300ml 100,000\\nè²´å·çœ\\nê·€ì£¼ì„±\\nì¥í–¥í˜•\\n53%\\nìˆ˜ìˆ˜ë¥¼ ì›ë£Œë¡œ í•˜ì—¬ 8ë²ˆì˜ ë°œíš¨ì™€ 9ë²ˆì˜ ì¦ë¥˜ë¥¼ ê±°ì¹œ í›„ 3ë…„ ì´ìƒ ìˆ™ì„±ì‹œí‚¨ ìˆ ë¡œ, ìì—° ê·¸ëŒ€ë¡œì˜ ë°œíš¨ê³µë²•ì„ í†µí•´ ì˜¤ì§ ì¢‹ì€ ì›ì¬ë£Œ ë§Œìœ¼ë¡œ ë¹šì–´ë‚¸ ì¤‘êµ­ì˜ ëŒ€í‘œ ëª…ì£¼\\n500ml 1,500,000\\në†í–¥í˜•\\n33%\\nì¤‘êµ­ ëª…ì£¼ë¥¼ ë§Œë“œëŠ” ê°• â€˜ì ìˆ˜í•˜â€™ì˜ ë§‘ì€ ë¬¼ê³¼ ì—„ì„ ëœ ë¶‰ì€ ìˆ˜ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì–‘ì¡°í•œ ìˆ ë¡œ, í’ë¶€í•œ í–¥ì´ ì§„í•œ ì—¬ìš´ì„ ë‚¨ê¸°ê³  ê¹”ë”í•œ ëª© ë„˜ê¹€ê³¼ ê¹Šì€ ë§›ì´ íŠ¹ì§•ì¸ ë°±ì£¼\\n450ml 100,000\\nå››å·çœ\\nì‚¬ì²œì„±\\në†í–¥í˜•\\n52% ì°¹ìŒ€, ìˆ˜ìˆ˜, , ì˜¥ìˆ˜ìˆ˜, ìŒ€ ë“± ë‹¤ì„¯ ê°€ì§€ ê³¡ë¬¼ì„ ì¬ë£Œë¡œ ë¹šì€ ìˆ ë¡œ, ë†’ì€ ë„ìˆ˜ì— ë¹„í•´ ë¶€ë“œëŸ½ê³  í’ë¶€í•œ ë§›ì´ ì¼í’ˆì¸ ì¤‘êµ­ì˜ ëŒ€í‘œ ëª…ì£¼\\n500ml 750,000\\në†í–¥í˜•\\n52% *2ì¸ ì´ìƒ ì£¼ë¬¸ ì‹œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\nëƒ‰ì±„ë¥˜\\níƒ•ë¥˜\\nì¡í’ˆë¥˜\\në°”ë‹·ê°€ì¬Â·ìƒˆìš°ë¥˜\\nì‡ ê³ ê¸°ë¥˜\\në¼ì§€ê³ ê¸°ë¥˜\\në‹­ê³ ê¸°ë¥˜\\nêµ¬ì´ë¥˜\\nì±„ì†ŒÂ·ë‘ë¶€ë¥˜\\në©´ë¥˜\\në°¥ë¥˜\\nSteamed Dim Sum\\nì¡°ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ë‹­ìœ¡ìˆ˜ìš© ë‹­ì€ êµ­ë‚´ì‚°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n ë§›ê³¼ ë§‘ê³  ê·¸ìœ½í•œ ì”í–¥ì´ íŠ¹ì§•ì¸ ë°±ì£¼\\n500ml 250,000\\në´‰í–¥í˜• 45% ì¤‘êµ­ ì–‘ê·€ë¹„ì˜ ìµœì• ìˆ ë¡œ ì—¬ì™•ì˜ ìˆ ì´ë¼ ë¶ˆë¦¬ë©°,\\në¶€ë“œëŸ½ê³  ë‹¬ì½¤í•œ ë§›ê³¼ ê¹”ë”í•˜ê³  ì²­ì•„í•œ í–¥ì˜ ì¡°í™”ê°€ ì¼í’ˆì¸ ë°±ì£¼\\n500ml 200,000\\nì²­í–¥í˜•\\n49% ë¶ˆìˆœë¬¼ì´ ì—†ê³  ê¹¨ë—í•œ ë§›ìœ¼ë¡œ ìœ ëª…í•œ ëŒ€ì¤‘ì ì¸ ìˆ ë¡œ, í¸ì•ˆí•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ë¶€ë“œëŸ¬ìš´ í–¥ì˜ ë°±ì£¼\\n280ml 45,000\\nå±±æ±çœ\\nì‚°ë™ì„±\\në†í–¥í˜• 52% ê³µìì˜ ì•½ì†ì´ë¼ëŠ” ëœ»ì„ ë²¨ë²³ ê°™ì€ í¸ì•ˆí•¨ê³¼ ë¶€ë“œëŸ¬ì›€ì´ ëŠê»´ì§€ëŠ” ê· í˜•ê°ì´ ì¢‹ì€ ë°±ì£¼\\n480ml 150,000\\në†í–¥í˜•\\n38% ì¤‘êµ­ì˜ 8ëŒ€ ëª…ì£¼ì´ì ì¤‘êµ­ì˜ ê±´ë¥­ì œê°€ ì¦ê²¨ë§ˆì‹œë˜ ìˆ ë¡œ, ë¶€ë“œëŸ½ê³  ê¸´ ì—¬ìš´ì´ íŠ¹ì§•ì¸ ë°±ì£¼\\n500ml 80,000\\nå±±è¥¿çœ Â· å¤©æ´¥\\nì‚°ì„œì„± Â· ì²œì§„\\në´‰í–¥í˜• 45% ì¤‘êµ­ 4ëŒ€ ëª…ì£¼ì´ì ì¤‘êµ­ ì§„ì‹œí™©ì´ ì¦ê²¨ ë§ˆì‹œë˜ ìˆ ë¡œ,\\n30ë…„ë™ì•ˆ ìˆ™ì„±ë˜ì–´ ê¹Šê³  ë¬µì§í•œ\",\n",
       " 'question': 'íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ì›¹ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì¤˜.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_agent_parallel.invoke(\n",
    "    {\n",
    "        \"input\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ì›¹ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì¤˜.\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb245a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI Assistant. You must answer the question based on the context.\\n#Context: {context}\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "eeve = ChatOllama(\n",
    "    model=\"EEVE-Korean-10.8B:long\",\n",
    "    temperature=0,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(question=itemgetter(\"question\"),\n",
    "                     context=itemgetter(\"context\"))\n",
    "    | prompt\n",
    "    | eeve\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e8b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = search_agent_parallel | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd187eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<|im_start|>system\n",
      "Perform the task to the best of your ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherise, you have access to the following tools:\n",
      "\n",
      "tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query., args: {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
      "and a `action_input` key (with the input to the tool going here).\n",
      "The only values that should be in the \"action\" field are: ['tavily_search_results_json']\n",
      "The $JSON_BLOB should only contain a SINGLE action, \n",
      "do NOT return a list of multiple actions.\n",
      "Here is an example of a valid $JSON_BLOB:\n",
      "```\n",
      "{\n",
      "    \"action\": $TOOL_NAME,\n",
      "    \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "The $JSON_BLOB must always be enclosed with triple backticks!\n",
      "\n",
      "ALWAYS use the following format:\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action:```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: the result of the action... \n",
      "(this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer` when responding.'\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Question: íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”\n",
      "Thought: I need to search for the phone number of 'íŒêµ ëª½ì¤‘í—Œ' using the tavily_search_results_json tool.\n",
      "Action:```{\n",
      "     \"action\": \"tavily_search_results_json\",\n",
      "     \"action_input\": {\n",
      "         \"query\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸\"\n",
      "     }\n",
      "}```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "íŒêµ ëª½ì¤‘í—Œì˜ ì „í™”ë²ˆí˜¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: 031-601-7574."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'íŒêµ ëª½ì¤‘í—Œì˜ ì „í™”ë²ˆí˜¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: 031-601-7574.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"íŒêµ ëª½ì¤‘í—Œ ì „í™”ë²ˆí˜¸ ê²€ìƒ‰í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
