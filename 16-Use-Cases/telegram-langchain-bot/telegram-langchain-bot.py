import os
from dotenv import load_dotenv
from langchain_text_splitters import Language
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import LanguageParser
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# Python íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
repo_root = "/home/hellocosmos/telegram-bot/langchain/libs"
repo_core = repo_root + "/core/langchain_core"
repo_community = repo_root + "/community/langchain_community"
repo_experimental = repo_root + "/experimental/langchain_experimental"
repo_partners = repo_root + "/partners"
repo_text_splitter = repo_root + "/text_splitters/langchain_text_splitters"
repo_cookbook = repo_root + "/cookbook"

py_documents = []
for path in [repo_core, repo_community, repo_experimental, repo_partners, repo_cookbook]:
    loader = GenericLoader.from_filesystem(
        path, glob="**/*", suffixes=[".py"],
        parser=LanguageParser(language=Language.PYTHON, parser_threshold=30),
    )
    py_documents.extend(loader.load())
print(f".py íŒŒì¼ì˜ ê°œìˆ˜: {len(py_documents)}")

py_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200
)
py_docs = py_splitter.split_documents(py_documents)
print(f"ë¶„í• ëœ .py íŒŒì¼ì˜ ê°œìˆ˜: {len(py_docs)}")

# MDX íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
root_dir = "/home/hellocosmos/telegram-bot/langchain/"

mdx_documents = []
for dirpath, dirnames, filenames in os.walk(root_dir):
    for file in filenames:
        if (file.endswith(".mdx")) and "*venv/" not in dirpath:
            try:
                loader = TextLoader(os.path.join(dirpath, file), encoding="utf-8")
                mdx_documents.extend(loader.load())
            except Exception:
                pass
print(f".mdx íŒŒì¼ì˜ ê°œìˆ˜: {len(mdx_documents)}")

mdx_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
mdx_docs = mdx_splitter.split_documents(mdx_documents)
print(f"ë¶„í• ëœ .mdx íŒŒì¼ì˜ ê°œìˆ˜: {len(mdx_docs)}")

# Teddyë‹˜ì˜ ë­ì²´ì¸ë…¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
import pandas as pd
from langchain.schema import Document

df = pd.read_csv('data_list_with_content.csv')
df_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
teddy_docs = []
for index, row in df.iterrows():
    if pd.isna(row['content']):
        continue
    chunks = df_splitter.split_text(row['content'])
    for chunk in chunks:
        teddy_docs.append(Document(page_content=chunk, metadata={"title": row['title'], "source": row['source']}))
print(f"ë¶„í• ëœ .df íŒŒì¼ ê°œìˆ˜: {len(teddy_docs)}")

# PDF íŒŒì¼ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• í•©ë‹ˆë‹¤. (PDF íŒŒì¼ì€ ìœ ë£Œêµ¬ë§¤í•˜ì…”ì•¼ í•©ë‹ˆë‹¤)
pdf_docs = []
document = PyPDFLoader("data/Generative_Al_with_LangChain.pdf").load_and_split()
pdf_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
pdf_docs = pdf_splitter.split_documents(document)
print(f"ë¶„í• ëœ .pdf íŒŒì¼ì˜ ê°œìˆ˜: {len(pdf_docs)}")

# íŒŒì´ì¬ ë¬¸ì„œ, MDX ë¬¸ì„œ, PDF ë¬¸ì„œ, í…Œë””ë…¸íŠ¸(Langhchin-KR) ë¬¸ì„œë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
combined_documents = teddy_docs + py_docs + mdx_docs + pdf_docs
print(f"ì´ ë„íë¨¼íŠ¸ ê°œìˆ˜: {len(combined_documents)}")

# í•„ìš”í•œ ì„ë² ë”©ê³¼ ìºì‹±ì„¤ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

store = LocalFileStore("./cache/")
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", disallowed_special=())
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=embeddings.model)

# Kiwi Tokenizerë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
from kiwipiepy import Kiwi

kiwi = Kiwi()
def kiwi_tokenize(text):
    return [token.form for token in kiwi.tokenize(text)]

# FAISS í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì™€ ê²€ìƒ‰ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
from langchain_community.vectorstores import FAISS, Chroma

FAISS_DB_INDEX = "langchain_faiss"
db = FAISS.from_documents(combined_documents, cached_embeddings)
db.save_local(folder_path=FAISS_DB_INDEX)
db = FAISS.load_local(FAISS_DB_INDEX, cached_embeddings, allow_dangerous_deserialization=True)
faiss_retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 10})

# BM25Retriever í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì™€ ê²€ìƒ‰ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
from langchain_community.retrievers import BM25Retriever

kiwi_bm25_retriever = BM25Retriever.from_documents(combined_documents, preprocess_func=kiwi_tokenize, k=10)

# EnsembleRetriever í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[kiwi_bm25_retriever, faiss_retriever],
    weights=[0.7, 0.3], search_type="mmr",
)

# PromptTemplateì„ ìƒì„±í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
"""
ë‹¹ì‹ ì€ 20ë…„ì°¨ AI ê°œë°œìì´ì íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ìµœëŒ€í•œ ë¬¸ì„œì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ì˜ ìˆ«ìê°€ ì íŒ ìˆœì„œëŒ€ë¡œ ì ˆì°¨ë¥¼ ì§€ì¼œì„œ ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•˜ê³  ì§„í–‰í•˜ì„¸ìš”.

1.ì£¼ì–´ì§„ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²½ìš°, "ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤" ë¼ê³  ì‹œì‘í•œë‹¤. Python ì½”ë“œì— ëŒ€í•œ ìƒì„¸í•œ code snippetì„ í¬í•¨í•´ì•¼ í•˜ë©°, ì½”ë“œ ì„¤ëª…ì— ëŒ€í•œ ì£¼ì„ë„ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ìì„¸í•˜ê²Œ ì„¤ëª…í•˜ê³ , í•œê¸€ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ëŠ” ê²½ìš° ì¶œì²˜(source)ë¥¼ ë°˜ë“œì‹œ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤. ì¶œì²˜ëŠ” ì ˆëŒ€ê²½ë¡œë¡œ ì¶œë ¥ë˜ëŠ” ê²½ìš° "/home/hellocosmos/telegram-bot"ì€ ìƒëµí•˜ê³  ì¶œë ¥í•´ì£¼ì„¸ìš”. ì¶œì²˜ê°€ PDF íŒŒì¼ì¸ ê²½ìš° "ì¶œì²˜ ì†ŒìŠ¤, í˜ì´ì§€"ë¥¼ í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤. ë©”íƒ€ë°ì´í„°ì˜ titleì´ ë¹ˆê³µë°±ì´ ì•„ë‹Œ ê²½ìš° ë°˜ë“œì‹œ "title, source" í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•´ì•¼ í•©ë‹ˆë‹¤.
2.ì£¼ì–´ì§„ ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ëŠ” AI, Langchain ë° íŒŒì´ì¬ ì „ë¬¸ê°€ë¡œì¨ ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ê´€ë ¨ ì§€ì‹ë§Œì„ í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤. "ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ì§€ë§Œ, ì•Œê³  ìˆëŠ” ì§€ì‹ì„ í™œìš©í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì‹œì‘í•œë‹¤. ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ë‹µë³€í•˜ê³ , ì¶œì²˜ëŠ” ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì¶œì²˜ë¥¼ í‘œê¸°í•´ì£¼ì„¸ìš”.
3.ì£¼ì–´ì§„ ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ê³ , Langchain ë° íŒŒì´ì¬ ì „ë¬¸ê°€ë¡œì¨ ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ê´€ë ¨ ì§€ì‹ë§Œì„ í™œìš©í•´ë„ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ëŠ” "AI ë° Langchainì— ëŒ€í•´ ë¬¸ì˜í•´ì£¼ì„¸ìš” ğŸ˜‚"ë¼ê³  ë‹µë³€í•´ì•¼ í•˜ë©°, ì¶œì²˜ëŠ” ìƒëµ í•´ì£¼ì„¸ìš”.

#ì°¸ê³ ë¬¸ì„œ:
{context}

#ì§ˆë¬¸:
{question}

#ë‹µë³€: 

#ì¶œì²˜:
- source1
- source2
- ...
"""
)

from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.callbacks.manager import CallbackManager
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

class StreamCallback(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        print(token, end="", flush=True)

# LLM ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.
llm = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True, callbacks=[StreamCallback()])

# Retriever ë¬¸ì„œë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
def format_docs(documents):
    formatted_list = []
    for doc in documents:
        title = doc.metadata.get('title', '')  # titleì´ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
        formatted_list.append(
            f"<doc><content>{doc.page_content}</content><title>{title}</title><source>{doc.metadata['source']}</source></doc>"
        )
    return formatted_list

# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
rag_chain = (
    {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
    | prompt | llm | StrOutputParser()
)

# ë‚ ì§œì™€ ì‹œê°„ í•¨ìˆ˜
from datetime import datetime

def get_current_datetime():
    now = datetime.now()
    formatted_datetime = now.strftime("%Y-%m-%d %H:%M:%S")
    return formatted_datetime

# í…”ë ˆê·¸ë¨ ë´‡ ì„¤ì • ë° í•¸ë“¤ëŸ¬ ì •ì˜
import telegram
from telegram.ext import Application, CommandHandler, MessageHandler, filters
from telegram.constants import ChatAction, ParseMode

# í…”ë ˆê·¸ë¨ ë´‡ í† í°ì„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
bot = telegram.Bot(os.getenv("BOT_TOKEN"))

# RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_response(message):
    return rag_chain.invoke(message)

# í…ìŠ¤íŠ¸ë¥¼ Telegram Markdown V2 í˜•ì‹ìœ¼ë¡œ ì´ìŠ¤ì¼€ì´í”„í•˜ëŠ” í•¨ìˆ˜
def escape_markdown_v2(text):
    escape_chars = r'\`*_{}[]()#+-.!|>='
    return ''.join(['\\' + char if char in escape_chars else char for char in text])

# ì‘ë‹µì„ ë‚˜ëˆ„ì–´ ë§ˆí¬ë‹¤ìš´ V2 í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
def split_response(response):
    parts = response.split("```")
    result = []
    for i, part in enumerate(parts):
        if i % 2 == 0:
            result.append(escape_markdown_v2(part))
        else:
            result.append(f"```{part}```")
    return result

# ë´‡ì˜ /start ëª…ë ¹ì— ëŒ€í•œ í•¸ë“¤ëŸ¬ í•¨ìˆ˜
async def start(update, context):
    await context.bot.send_message(chat_id=update.effective_chat.id, text="ì•ˆë…•í•˜ì„¸ìš”, Langchain ì±—ë´‡ì…ë‹ˆë‹¤! ğŸ§‘â€ğŸ’»")

# í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¸ë“¤ëŸ¬ í•¨ìˆ˜
async def answer_openai(update, context):
    message = update.message.text

    # ìœ ì € ì´ë¦„ ë˜ëŠ” ì‚¬ìš©ìëª… ì¶”ì¶œ
    user = update.message.from_user  # ìœ ì € ì •ë³´ ì¶”ì¶œ
    user_id = update.message.from_user.id  # ìœ ì € ID ì¶”ì¶œ
    user_identifier = user.username if user.username else f"{user.first_name} {user.last_name if user.last_name else ''}"
    date_time = get_current_datetime()
    print(f"\n[User_Info] uid: {user_id},  name: {user_identifier}, date: {date_time}")
    print(f"\n[Question] {message}\n[Answer]\n")    

    chat_id = update.effective_chat.id

    loading_message = await context.bot.send_message(chat_id=chat_id, text="ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤... ğŸ§‘â€ğŸ’»")
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
    try:
        response = generate_response(message)
        print("\n\n")
    except Exception as e:
        await context.bot.delete_message(chat_id=chat_id, message_id=loading_message.message_id)
        await context.bot.send_message(chat_id=chat_id, text=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return
    
    await context.bot.delete_message(chat_id=chat_id, message_id=loading_message.message_id)
    
    # ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ê³  ë§ˆí¬ë‹¤ìš´ V2 ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
    formatted_response_parts = split_response(response)
    
    # ë””ë²„psê¹… ì¶œë ¥ì„ ì¶”ê°€í•˜ì—¬ ì´ìŠ¤ì¼€ì´í”„ëœ í…ìŠ¤íŠ¸ í™•ì¸
    # for part in formatted_response_parts: print(part)
    
    # ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ë©´ ë‚˜ëˆ„ì–´ì„œ ë³´ë‚´ê¸°
    for part in formatted_response_parts:
        if part.strip():  # partê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë©”ì‹œì§€ ì „ì†¡
            await context.bot.send_message(chat_id=update.effective_chat.id, text=part, parse_mode=ParseMode.MARKDOWN_V2)
# í…”ë ˆê·¸ë¨ ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ë° í•¸ë“¤ëŸ¬ ì¶”ê°€

application = Application.builder().token(os.getenv("BOT_TOKEN")).build()
application.add_handler(CommandHandler('start', start))
application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, answer_openai))

# ë´‡ ì‹¤í–‰
application.run_polling()